# CS-441 HOMEWORK 2

* First Name : Giuseppe
* Last name : Calderonio
* UIC email : gcalde22@uic.edu
* UIN: 679346611
* deployment video on youtube : [TODO]()

## Introduction
This repository contains the implementation of a Client Server
architecture implemented with scala and the gRPC protobuf 
framework (Book option 1), and specifically
implemented for the second homework of the CS-441 Cloud Computing
class of University of Illinois at Chicago

## Requirements

In order to correctly download and use the project, it's required
to have the following tools installed :

1. Scala 2.13.9
2. an AWS account

Note that with scala 3.* the code won't compile because it is
not supported yet by the akka http library used


## Problem statement

The problem consist on implementing three programs :
1. A program that generates random Log Files and stores them
   in a s3 AWS bucket that has to be deployed as an AWS EC2 instance
2. A gRPC server deployed as a lambda that receives in input two 
   timestamps ts, Delta_ts in the form ```HH:mm:ss.SSS```, one representing a single
   timestamp and one representing a delta to be summed with the first one,
   and has to search for log messages that both belong to the time
   interval ```[ts, ts + delta_ts]``` and match a regexp patter
   specified in a [configuration file](https://github.com/GiuseppeCalderonio/CS441_Homework2/blob/master/src/main/resources/application.conf)
   and return an http status code that represent either
   success or failure
3. A gRPC client that does the takes as input the two
   timestamps ts, Delta_ts in the form ```HH:mm:ss.SSS```, 
   sends them over HTTP to the lambda function with an RPC call,
   and receives the response

## Project description

### Configuration parameters

A [configuration file](https://github.com/GiuseppeCalderonio/CS441_Homework2/blob/master/src/main/resources/application.conf)
has been used to assign values to the parameters of the project
that may change frequently over time such as
the aws url, the regex pattern, and so on...
Then, using the library TypeSafe listed in the [build.sbt fle](https://github.com/GiuseppeCalderonio/CS441_Homework2/blob/master/build.sbt)
a class called [Parameters](https://github.com/GiuseppeCalderonio/CS441_Homework2/blob/master/src/main/scala/HelperUtils/Parameters.scala)
that takes those values from the config file,
parses them and exposes them as the public interface of
the class.

### Protobuf

In order to implement gRPC client and servers, the protobuf
serialization protocol has been used, in particular

1. **TimestampRequest** represents the input for the
protobuf request, it contains two strings that represent respectively
the timestamp and the delta timestamp ti execute the search
2. **TimestampResponse** represents the output for the
protobuf response, it contains one string that give information
about the search outcome
3. **Search** represents the service that the protobuf offers,
in particular it contains the RPC interface of the method
   ```IsTimeIntervalPresent```that takes a ```TimestampRequest```
as input and returns a ```TimestampResponse```

More info can be found [here](https://github.com/GiuseppeCalderonio/CS441_Homework2/blob/master/src/main/protobuf/RemoteRPC.proto)

### Inputs

The inputs of the program can be divided for all the three
programs that have to be implemented

1. **gRPC client inputs** are two strings that should represent
   two
   timestamps ts, Delta_ts in the form ```HH:mm:ss.SSS```, one representing a single
   timestamp and one representing a delta to be summed with the first one.

2. **gRPC server /Lambda inputs** are the HTTP requests supposed
 to have a body containing the serialized protobuf message with
timestamps, and an input file generated by the EC2 Log generator.
3. **EC2 Log file generator inputs** are listed in the [configuration
parameters](https://github.com/GiuseppeCalderonio/LogFileGenerator/blob/master/src/main/resources/application.conf)
of the [Log File Generator](https://github.com/GiuseppeCalderonio/LogFileGenerator) Project

### Outputs

The inputs of the program can be divided for all the three
programs that have to be implemented

1. **gRPC client output** is a string that describes the outcome
of the remote procedure call, it can be either POSITIVE (log messages
were found),
NEGATIVE (log messages were not found), ERROR (an error occurred 
during the RPC call)
2. **gRPC server /Lambda output** is a status code, that can be 
either 200 for a POSITIVE response, or 480 for a negative response
3. **EC2 Log file generator output** is a file containing log
messages stored in an AWS S3 bucket


### Code

The code has been divided in two separate projects, one for
the [RPC implementation](https://github.com/GiuseppeCalderonio/CS441_Homework2) (this repository)
, and one for the
[Log File Generator implementation](https://github.com/GiuseppeCalderonio/LogFileGenerator)

#### RPC project


The RPC project is divided in 4 main packages :

1. [AWSLambda](https://github.com/GiuseppeCalderonio/CS441_Homework2/blob/master/src/main/scala/AWSLambda) :
   this package contains the classes that implement the 
   Lambda function, in particular those who get the file from the
   AWS S3 bucket, and that call the methods to execute the search 
   (which are implemented in the Timestamp package)
2. [Client](https://github.com/GiuseppeCalderonio/CS441_Homework2/blob/master/src/main/scala/Client) : 
   this package contains the classes that implement the gRPC
   client, in particular those who call the RPC and send the
   request over HTTP marshalling the timestamps
3. [HelperUtils](https://github.com/GiuseppeCalderonio/CS441_Homework2/blob/master/src/main/scala/HelperUtils) : 
   this package contains a set of classes that are useful for
   logging and for obtaining the configuration parameters
   at runtime
4. [Timestamp](https://github.com/GiuseppeCalderonio/CS441_Homework2/blob/master/src/main/scala/Timestamp) : 
   this package contains a set of helper functions for the
   timestamps such as summing two timestamps, parsing a timestamp
   from a string and, most importantly, binary search and filtering
   log messages for regex pattern.

For more info the code of those classes is commented in
details.


To run the gRPC client (which is the only
main file of the project)
, go to the root directory of the
project and run the following command :

```
sbt clean compile "run HH:mm:ss.SSS HH:mm:ss.SSS"
```

where the first ```HH:mm:ss.SSS``` represents the
first timestamp, and the second
```HH:mm:ss.SSS``` represents the delta timestamp


#### Log file generator project

the [Log File Generator project](https://github.com/GiuseppeCalderonio/LogFileGenerator)
was modified in such a way that now it also writes the content of
the generated file ```input.log``` in the AWS S3 bucket 
specified in the [configuration parameter](https://github.com/GiuseppeCalderonio/LogFileGenerator/blob/master/src/main/resources/application.conf)
file.

To run the log file generator
, go to the root directory of the
project and run the following command :

```
sbt clean compile run
```

## Testing

In order to test the application, 
[two test classes](https://github.com/GiuseppeCalderonio/CS441_Homework2/blob/master/src/test/scala/Timestamp)
have been implemented to verify the correctness
of the binary search and the timestamp
helper functions.

To run the test, go to the root directory of the
project and run the following command :

```
sbt clean compile test
```

## Deployment

### Jar file

In order to do a local deployment, the
**Assembly** sbt plugin has been used, that allows
to resolve dependencies and create a single jar from
both the two big projects.

In order to create the jar file go to the root
of the project and run the following command :
```
sbt clean compile assembly
```
Then go to the directory
1. **target/scala-2.13/** for the gRPC project
2. **target/scala-3.0.2/** for the log file generator project
   and the jar file should have been produced there.

### Local

In order to run locally the jar file this
command can be used :

1. for the gRPC project
```
java - jar <jarFileName> HH:mm:ss.SSS
```
2. for the log generator project
```
java - jar <jarFileName>
```

where _jarFileName_ is the location of the jar
file locally

### AWS deployment

#### Preliminary steps

Before diving in the AWS deploying part, it is necessary to
set up the proper security environment to execute the 
functions on the cloud.

In particular :

1. Create a **<Secret_key, Access_key>** pair following
   [this tutorial](https://k21academy.com/amazon-web-services/create-access-and-secret-keys-in-aws/)
   make sure to store in a safe location the resulting file.
   Once completed this step, make sure to include these values
   when creating the jar in the configuration files of both projects
   (entry is left blank)
2. Create a **Key Pair** following
   [this tutorial](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/create-key-pairs.html)
   make sure to store in a safe location the resulting file.
   also make sure to create a _.pem_ file and not a _.kkp_ file 

#### EC2 Log File Generator

In order to run the Log file generator on an EC2 instance,
the following has to be done :

1. Launch a new instance in the EC2 console with the _ubuntu_ image
   selected and selecting the _Key pair_ created in the 2-nd
   preliminary step
2. Send the jar file in the EC2 instance using the following command :
```
scp -i "<keyPairFileLocation>" <jarFileName> ubuntu@<instanceUrl>:hw2.jar
```
where _keyPairFileLocation_ is the location of the file downloaded
in the 2-nd preliminary step, _instanceUrl_ is the url
assigned to the instance that can be found on the instance details
and _jarFileName_ is the location of the jar
file locally
This command will create in the EC2 instance a jar file named
_hw2.jar_

3. Connect through ssh using the following command :
```
ssh -i "<keyPairFileLocation>" ubuntu@<instanceUrl>
```

where _keyPairFileLocation_ is the location of the file downloaded
in the 2-nd preliminary step, while _instanceUrl_ is the url
assigned to the instance that can be found on the instance details

4. Install java typing the following command :

```
sudo apt install openjdk-11-jre-headless
```

5. Run the jar file with the following command :
```
java -jar hw2.jar
```

and the output log file should be stored in the AWS S3 bucket as
**input.log**.


#### AWS Lambda

In order to deploy the project using a lambda function,
execute the following steps

1. Create a lambda function choosing the runtime _Java 11 (Corretto)_
   and the _x86_64_ architecture
2. Create a new trigger clicking on _add Trigger_, select the option _API Gateway_ clicking on the
   checkbox _Create a new API_
3. Click on the _API Gateway_ just created, copy the url
   provided, and paste it in the url entry of the [configuration file](https://github.com/GiuseppeCalderonio/CS441_Homework2/blob/master/src/main/resources/application.conf)
4. Upload the jar file (if the jar file is
 too big, it should be uploaded from an S3 bucket)
5. Set the handler of the request as
```
AWSLambda.LambdaHandler::handleRequest
```
6. Go in general configuration and increase the timeout to one
 minute (the read from S3 bucket requires more than 15 seconds
 which is the default)
After these steps, the lambda function is up and running, and
it is possible to invoke it with a gRPC client


### Video

Here can be found a 
[youtube video]()
where most of the steps are documented
